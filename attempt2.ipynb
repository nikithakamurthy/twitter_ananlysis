{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from techniques1 import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "    \n",
    "    arr=[]\n",
    "    with open(filename,'rt',encoding='latin-1') as file:     \n",
    "                samples=csv.reader(file)\n",
    "                x=0\n",
    "                for i in samples:\n",
    "                    if x==0:\n",
    "                        x=1\n",
    "                        continue\n",
    "                    arr.append(i)\n",
    "    df=pd.DataFrame(data=arr,columns=[\"types\",\"posts\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelencode(df):\n",
    "    data=df['types']\n",
    "    values=array(data)\n",
    "    label=LabelEncoder()\n",
    "    intencode=label.fit_transform(values)\n",
    "    df['typeint']=intencode\n",
    "    #df['typeint'].plot(kind='hist')\n",
    "    #k=np.arange(0,16)\n",
    "    #x=label.inverse_transform(k)   #can access encoded actual value using x\n",
    "    #print(values)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSlangframe():\n",
    "    f=\"slang.txt\"\n",
    "    a=[]\n",
    "    with open(f,'rt',encoding='latin-1') as file:     \n",
    "                samples=csv.reader(file,delimiter='\\t')\n",
    "                x=0\n",
    "                for i in samples:\n",
    "                    if x==0:\n",
    "                        x=1\n",
    "                        continue\n",
    "                    a.append(i)\n",
    "    slangframe=pd.DataFrame(data=a,columns=['slang','fullform'])\n",
    "    return slangframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(shortdata,slangframe):\n",
    "    slang=list(slangframe['slang'])\n",
    "    for i in range(len(shortdata)):\n",
    "        #counts emojis and remove\n",
    "        shortdata.loc[i,'emojicount']=countEmoticons(shortdata['posts'][i])\n",
    "        shortdata.loc[i,'posts']=removeEmoticons(shortdata['posts'][i])\n",
    "        #counts slang and removes\n",
    "        shortdata.loc[i,'slangcount']=countSlang(shortdata['posts'][i])\n",
    "        q=shortdata['posts'][i].split()\n",
    "        for j in range(len(q)):\n",
    "            if q[j] in slang:\n",
    "                q[j]=slangframe['fullform'][slang.index(q[j])]\n",
    "        shortdata.loc[i,'posts']=' '.join(q)\n",
    "        #replaces contractions with full forms\n",
    "        shortdata.loc[i,'posts']=replaceContraction(shortdata['posts'][i])\n",
    "        #counts urls and replaces\n",
    "        shortdata.loc[i,'urlcount']=countURL(shortdata['posts'][i])\n",
    "        shortdata.loc[i,'posts']=replaceurl(shortdata['posts'][i]) \n",
    "        #counts #()-() and @-atuser and removes them\n",
    "        shortdata.loc[i,'#_@count']=counthashAt(shortdata['posts'][i])\n",
    "        shortdata.loc[i,'posts']=replaceAtUser(shortdata['posts'][i])\n",
    "        shortdata.loc[i,'posts']=removeHashtagInFrontOfWord(shortdata['posts'][i])\n",
    "        #puctuations clearing\n",
    "        punct=clear_punctuation(shortdata['posts'][i])\n",
    "        shortdata.loc[i,'posts']=punct[0]\n",
    "        shortdata.loc[i,'punct_count']=punct[1]\n",
    "        #replaces negations\n",
    "        shortdata.loc[i,'posts']=replaceNegations(shortdata['posts'][i]) \n",
    "        #remove unicodes\n",
    "        shortdata.loc[i,'posts']=removeUnicode(shortdata['posts'][i]) \n",
    "        #removes numbers\n",
    "        shortdata.loc[i,'posts']=removeNumbers(shortdata['posts'][i])\n",
    "        #remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop=stopwords.words(\"english\")\n",
    "    my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm httpurl httpsurl http\" # my extra stopwords\n",
    "    stop = stop + my_stopwords.split()\n",
    "    shortdata['posts']=shortdata['posts'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    ps = PorterStemmer()\n",
    "    shortdata['posts'] = shortdata['posts'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    shortdata['posts'] = shortdata['posts'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "    #spell correction\n",
    "    \"\"\"for i in range(len(shortdata)):\n",
    "        q=shortdata.loc[i,'posts'].split()\n",
    "        for j in range(len(q)):\n",
    "            q[j]=spellCorrection(q[j])\n",
    "        shortdata.loc[i,'posts']=' '.join(q)\"\"\"\n",
    "    return shortdata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  types                                              posts  typeint  \\\n",
      "0  INFJ  intj moment http sportscent top ten play http ...        8   \n",
      "1  ENTP  Im find lack post alarmingsex bore posit often...        3   \n",
      "2  INTP  good one http cours I say I know bless cursedo...       11   \n",
      "3  INTJ  dear intp I enjoy convers day esoter gab natur...       10   \n",
      "4  ENTJ  you firedthat anoth silli misconcept that appr...        2   \n",
      "\n",
      "   emojicount  slangcount  urlcount  #_@count  punct_count  \n",
      "0        34.0         2.0      14.0       0.0        291.0  \n",
      "1        29.0         1.0       1.0       0.0        458.0  \n",
      "2        19.0         0.0       4.0       0.0        339.0  \n",
      "3         6.0         2.0       2.0       0.0        398.0  \n",
      "4        16.0         6.0       2.0       2.0        354.0  \n"
     ]
    }
   ],
   "source": [
    "def call_clean():\n",
    "    filename=\"mbti_1.csv\"\n",
    "    df=readfile(filename)\n",
    "    df=labelencode(df)\n",
    "    slangframe=createSlangframe()\n",
    "    clean_data=preprocess(df,slangframe)\n",
    "    print(clean_data.head())\n",
    "    return clean_data\n",
    "data=call_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data=np.array(data['posts'])\n",
    "label=np.array(data['typeint'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 152.,  540.,  185.,  548.,   34.,   38.,   31.,   71., 1176.,\n",
       "        1465.,  873., 1043.,  133.,  217.,  434.]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]),\n",
       " <a list of 15 Patch objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainset, testset, trainlabel, testlabel = train_test_split(proc_data, label,stratify=label, test_size=0.2)\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "#plt.plot(x)\n",
    "pyplot.hist(trainlabel,np.arange(0,16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(trainset)]\n",
    "#print(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,alpha=alpha, min_alpha=0.00025,min_count=1,dm =1)\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    #print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")\n",
    "print(model)\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[]\n",
    "for i in range(len(trainset)):\n",
    "    train.append(model.docvecs[i])\n",
    "trainset=np.array(train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18914373 -0.20302776  0.00416092 ...  0.18720366  0.22891383\n",
      "   0.05284937]\n",
      " [-0.3249938  -1.1973047   0.7114654  ...  0.8228047   0.62431777\n",
      "   0.5789972 ]\n",
      " [-0.00142897 -0.9997056   0.4400209  ...  0.76707894  0.94090474\n",
      "   1.0851221 ]\n",
      " ...\n",
      " [-0.41284165 -0.4688043   0.37761092 ...  0.4504887   0.44820946\n",
      "   0.33290735]\n",
      " [-0.21763164 -0.32383454  0.24509476 ...  0.7357502   0.5944933\n",
      "   0.48881269]\n",
      " [-0.46942854 -0.54872257  0.84596306 ...  0.22824056  0.9934551\n",
      "   0.40817428]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c87eb15ac84e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#to find the vector of a document which is not in training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#print(\"V1_infer\", v1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "test=[]\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "for i in testset:\n",
    "    test_data = word_tokenize(i.lower())\n",
    "    test.append(model.infer_vector(test_data))\n",
    "#print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "#similar_doc = model.docvecs.most_similar('0')\n",
    "#print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "testset=np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4681556195965418\n",
      "accuracy= 16.59942363112392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model=KNeighborsClassifier(n_neighbors=6)\n",
    "model.fit(trainset,trainlabel)\n",
    "print(model.score(train,trainlabel))\n",
    "\n",
    "predictionsknn = model.predict(testset)\n",
    "count=0\n",
    "for i in range(len(predictionsknn)):\n",
    "    if predictionsknn[i]==testlabel[i]:\n",
    "        count=count+1\n",
    "print(\"accuracy=\",(count*100)/len(testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3527377521613833\n",
      "accuracy= 17.23342939481268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model=GaussianNB()\n",
    "model.fit(trainset,trainlabel)\n",
    "print(model.score(trainset,trainlabel))\n",
    "predictionsgnb=model.predict(testset)\n",
    "predictionsgnb = model.predict(testset)\n",
    "count=0\n",
    "for i in range(len(predictionsgnb)):\n",
    "    if predictionsgnb[i]==testlabel[i]:\n",
    "        count=count+1\n",
    "print(\"accuracy=\",(count*100)/len(testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47521613832853027\n",
      "accuracy= 20.461095100864554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model=MLPClassifier()\n",
    "model.fit(trainset,trainlabel)\n",
    "print(model.score(trainset,trainlabel))\n",
    "predictionsmlp=model.predict(testset)\n",
    "predictionsmlp = model.predict(testset)\n",
    "count=0\n",
    "for i in range(len(predictionsmlp)):\n",
    "    if predictionsmlp[i]==testlabel[i]:\n",
    "        count=count+1\n",
    "print(\"accuracy=\",(count*100)/len(testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30403458213256485\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-12c05f994d97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mprediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \"\"\"\n\u001b[0;32m    952\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coefs_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \"\"\"\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;31m# Make sure self.hidden_layer_sizes is a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "kf = KFold(n_splits=10)\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "for train_indices, test_indices in kf.split(trainset):\n",
    "    clf.fit(trainset[train_indices], trainlabel[train_indices])\n",
    "    #print(clf.score(trainset[test_indices], trainlabel[test_indices]))\n",
    "    prediction=clf.predict(test)\n",
    "    c=0\n",
    "    for i in range(len(prediction)):\n",
    "        if predictionsmlp[i]==testlabel[i]:\n",
    "        c=c+1\n",
    "    print(\"accuracy=\",c*100/len(testlabel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
