{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk.data\n",
    "import csv\n",
    "from nltk.stem.porter import *\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import array as arr\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'downloaded1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4f17eb9eb18a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloaded1.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'downloaded1.csv'"
     ]
    }
   ],
   "source": [
    "arr=[]\n",
    "file = open(\"downloaded1.csv\",'rt')\n",
    "samples=csv.reader(file)\n",
    "c=0\n",
    "for i in samples:\n",
    "    c+=1\n",
    "    \n",
    "    if c==2:\n",
    "        x=i[1]\n",
    "        break\n",
    "\n",
    "for i in samples:\n",
    "    if i[1]!=x:\n",
    "        arr.append(i)\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data=arr,columns=(\"types\",\"posts\"))\n",
    "print(len(df.columns))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelencode(df):\n",
    "    data=df['types']\n",
    "    values=np.array(data)\n",
    "    label=LabelEncoder()\n",
    "    intencode=label.fit_transform(values)\n",
    "    df['typeint']=intencode\n",
    "    print(list(label.inverse_transform([0,1,2])))\n",
    "    #df['typeint'].plot(kind='hist')\n",
    "    #k=np.arange(0,16)\n",
    "    #x=label.inverse_transform(k)   #can access encoded actual value using x\n",
    "    #print(values)\n",
    "    return df\n",
    "\n",
    "df=labelencode(df)\n",
    "#print(df)\n",
    "shortdata=df.iloc[:,1]\n",
    "#print(shortdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words(\"english\")\n",
    "print('------Removing stopwords------')\n",
    "shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word!='i' or word!='I']))\n",
    "print(shortdata)\n",
    "#stemming of words\n",
    "ps = PorterStemmer()\n",
    "print('-------Stemming--------')\n",
    "shortdata = shortdata.apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "print(shortdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shortdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2100de8702fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshortdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshortdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlmtzr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(shortdata)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shortdata' is not defined"
     ]
    }
   ],
   "source": [
    "shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "#print(shortdata)\n",
    "print('-------Lemmatization--------')\n",
    "shortdata = shortdata.apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "print(shortdata)\n",
    "\n",
    "print('--------Removing punctuations--------')\n",
    "def clear_punctuation(s):\n",
    "\timport string\n",
    "\t#print(\"\\n\")\n",
    "\tclear_string = \"\"\n",
    "\tfor symbol in s:\n",
    "\t\tif symbol not in string.punctuation:\n",
    "\t\t\tclear_string += symbol\n",
    "\treturn clear_string\n",
    "\n",
    "shortdata = shortdata.apply(lambda x: ''.join(clear_punctuation(x))  )\n",
    "print(shortdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_all_entities(text):\n",
    "\timport string\n",
    "\tentity_prefixes = ['@']\n",
    "\tfor separator in  string.punctuation:\n",
    "\t\tif separator not in entity_prefixes :\n",
    "\t\t\ttext = text.replace(separator,' ')\n",
    "\twords = []\n",
    "\tfor word in text.split():\n",
    "\t\tword = word.strip()\n",
    "\t\tif word:\n",
    "\t\t\tif word[0] not in entity_prefixes:\n",
    "\t\t\t\twords.append(word)\n",
    "\treturn ' '.join(words)\n",
    "\n",
    "shortdata = shortdata.apply(lambda x: ''.join(strip_all_entities(x))  ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----PREPROCESSED_DATA------\n",
      "         types                                              posts  typeint\n",
      "0     negative  iranian gener say iron dome deal missil talk l...        0\n",
      "1     positive  J davlar main rival team hope make success end...        2\n",
      "2     negative  talk decid I want go appli colleg everyth coll...        0\n",
      "3     negative       they may superbowl dalla win not quarterback        0\n",
      "4      neutral      Im bring monster load candi I hope get squich        1\n",
      "5      neutral  appl retail chief san francisco appl inc ceo t...        1\n",
      "6     positive                      I watch U rememb sun morn nta        2\n",
      "7      neutral   nadal confirm mexican open rafael nadal set play        1\n",
      "8     positive  I didnt want pop yep chapel hill next wednesda...        2\n",
      "9      neutral  hmmmm novemb odd releas date true becom big en...        1\n",
      "10     neutral  US delist mko global terrorist list line iran ...        1\n",
      "11    positive                good morn becki thursday go fantast        2\n",
      "12     neutral  expect rain samar leyt chanc rain expect fair ...        1\n",
      "13    positive  one ticket leave game miss rematch nfc champio...        2\n",
      "14    negative          afc away fan all stuff say when turn back        0\n",
      "15     neutral  game nlc rematch nfc championship game gonna c...        1\n",
      "16    positive   never start work dream goal never mean anyth act        2\n",
      "17    positive      I vick need vick may get jen back I think win        2\n",
      "18    positive          look like andi android may littl much fun        2\n",
      "19    positive       oooh nice ti tempt go lake nikon hmmmm I may        2\n",
      "20     neutral  black friday huge save aerial view pari la la ...        1\n",
      "21    negative                           jenel say alon say weird        0\n",
      "22     neutral  moham muslim brotherhood instruct suprem counc...        1\n",
      "23     neutral  you get tag guy look like kid bill murray rese...        1\n",
      "24     neutral  At first grammi hold may domenico modugno beat...        1\n",
      "25    positive          I studi day tomorrow go omg jennett gonna        2\n",
      "26    positive  good morn guarante tomorrow give everyth tim cook        2\n",
      "27    positive   mcfli come back argentina time want come mar del        2\n",
      "28     neutral   peyton man name afc offens player second tom tie        1\n",
      "29     neutral                    bring kendrick lamar get ticket        1\n",
      "...        ...                                                ...      ...\n",
      "6906  positive        hope win tonight your US worldwid fan cheer        2\n",
      "6907  positive                       RT monday night footbal move        2\n",
      "6908   neutral  colt owner jim irsay give victim fund state fa...        1\n",
      "6909  positive  If read twitter trend list peopl realli happi ...        2\n",
      "6910  positive                                          colt game        2\n",
      "6911   neutral  Is tv most redskin starter sit tonight vs tamp...        1\n",
      "6912  positive  what better TV v tonight mnf reg season hardba...        2\n",
      "6913  positive  readi colleg bring make monday We come home yo...        2\n",
      "6914  positive  tri minut late still need dash ladi wait hear dwt        2\n",
      "6915  negative    I fail see ram play twice mnf div game isnt til        0\n",
      "6916  negative  I hope iphon get servic tonit find dwt caus I ...        0\n",
      "6917   neutral                         On night hank william come        1\n",
      "6918  positive  RT there parti usa tonight parti worldwid mile...        2\n",
      "6919  positive  man jet cowboy game awesom great entertain tha...        2\n",
      "6920  positive                                        mnf go sexi        2\n",
      "6921  negative  lmao RT curti painter look like kind guy would...        0\n",
      "6922   neutral                           monday night footbal day        1\n",
      "6923   neutral  come watch those guy softbal next saturday aug...        1\n",
      "6924  negative  hank william whiski empti bar guy cute bartend...        0\n",
      "6925  positive  love keep touch one favorit dwt coupl reason I...        2\n",
      "6926  positive   huge thank come photographi workshop today terra        2\n",
      "6927  negative   RT trend worldwid thi not someth proud unit sort        0\n",
      "6928  positive  monday night footbal gari nevil well even time...        2\n",
      "6929  positive  the tiger know big game rest martinez alburque...        2\n",
      "6930  positive  new cast dwt tba So excit the meet tonight bet...        2\n",
      "6931  negative                        recommend turn wait verland        0\n",
      "6932  positive             RT monday monday night footbal RT love        2\n",
      "6933  positive  all I know road lomardi start We set record mn...        2\n",
      "6934   neutral  all blue white r meet golden corral dinner ton...        1\n",
      "6935  positive                   have great game agaist tampa bay        2\n",
      "\n",
      "[6936 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "arr=[]\n",
    "print(\"-----PREPROCESSED_DATA------\")\n",
    "count=0\n",
    "for line in shortdata:\n",
    "    df.iloc[i,1]=line\n",
    "    i=i+1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936\n",
      "6936\n"
     ]
    }
   ],
   "source": [
    "proc_data=np.array(df['posts'])\n",
    "label=np.array(df['typeint'])\n",
    "print(len(proc_data))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=0.1,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.1, random_state=0)\n",
    "sss.get_n_splits(proc_data, label)\n",
    "print(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6242\n",
      "694\n",
      "6242\n",
      "694\n",
      "[1 2 2 2 1 1 1 1 1 1 1 2 0 2 2 1 1 2 0 0 1 2 2 1 2 1 2 2 2 2 1 1 1 2 2 2 2\n",
      " 1 2 2 2 2 1 2 1 0 1 0 1 2 1 0 1 1 1 1 0 1 0 1 2 1 1 0 2 1 1 1 1 1 2 2 1 1\n",
      " 2 1 1 0 2 1 1 2 1 1 1 1 1 2 1 2 2 1 1 1 1 0 1 2 1 1 1 1 0 2 2 2 1 1 1 0 2\n",
      " 2 0 1 2 2 2 2 2 1 1 1 1 2 2 1 1 1 2 1 1 2 1 2 1 2 1 1 2 1 1 2 2 2 1 1 1 1\n",
      " 1 1 2 0 1 0 2 0 0 1 1 2 1 1 2 2 1 1 1 1 2 2 0 1 1 2 2 1 2 2 2 2 1 1 0 1 2\n",
      " 1 1 1 1 2 1 0 1 1 2 1 1 2 1 2 0 2 2 2 0 2 1 1 1 1 1 1 1 2 2 1 1 2 1 1 1 1\n",
      " 2 1 0 1 1 1 1 2 2 1 1 2 0 0 1 2 1 1 2 1 1 0 1 2 1 1 0 2 0 0 2 2 2 2 1 2 2\n",
      " 2 2 1 0 1 2 2 1 1 1 1 1 0 2 2 0 1 2 2 2 2 2 2 1 0 1 2 2 1 0 1 1 2 0 2 2 1\n",
      " 2 1 2 2 1 0 2 1 1 1 1 1 0 1 2 0 2 1 0 2 2 2 1 2 1 1 1 1 0 1 2 1 1 1 2 1 1\n",
      " 2 0 0 1 2 1 1 2 0 2 2 2 1 1 2 2 1 2 1 1 2 2 2 1 1 1 2 2 2 2 0 0 0 1 2 1 2\n",
      " 2 2 1 2 2 2 1 2 2 1 1 1 2 1 1 1 1 1 0 0 1 0 1 1 0 2 1 2 1 2 0 1 1 1 0 1 1\n",
      " 1 0 0 1 0 1 0 2 2 1 1 2 0 2 1 2 2 0 1 1 2 1 1 1 1 2 2 1 1 1 0 2 2 2 2 1 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 0 0 1 2 2 1 1 2 0 1 2 2 1 1 0 0 2 1 2\n",
      " 2 2 1 2 1 1 0 2 2 1 1 1 2 2 1 2 2 2 1 2 2 2 1 2 2 1 2 1 1 2 0 2 1 2 0 2 1\n",
      " 1 1 1 2 1 0 1 1 2 1 2 1 1 2 1 2 1 2 1 2 0 0 2 0 2 0 1 2 1 2 0 2 2 2 1 2 1\n",
      " 2 1 1 1 0 2 2 0 1 2 1 0 2 2 2 1 2 0 2 2 0 1 2 1 0 2 1 1 2 2 1 1 1 2 2 1 1\n",
      " 2 2 2 1 2 2 2 1 2 1 1 1 1 1 2 1 0 2 1 0 1 1 2 1 1 1 2 2 2 1 1 2 0 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 0 2 0 1 0 0 1 0 1 2 2 1 2 1 1 2 1 0 2 1 2 1 2 1 2 1 1 1 2\n",
      " 2 1 1 2 2 1 1 2 2 2 2 0 1 0 0 0 0 1 2 1 1 2 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(proc_data, label,\n",
    "                                                    stratify=label, \n",
    "                                                    test_size=0.1)\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6242\n",
      "['My daili tweet memori trayvon kill day rest hope'\n",
      " 'wow last year sinc captain make start afc time'\n",
      " 'prayer amaz men god groceri arriv pokemon purchas big first' ...\n",
      " 'keith richard mean noth monday night cant forgiv hank'\n",
      " 'decent woot deal HD till via'\n",
      " 'bjp member walk jpc meet bharatiya janata parti member tuesday walk']\n"
     ]
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in X_train]\n",
    "#print(texts)\n",
    "print(len(X_train))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.38782084 0.18078431 0.25367609 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "#print(processed_corpus)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "from gensim import models\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "train=np.zeros((len(X_train),len(dictionary)))\n",
    "count=0\n",
    "for text in X_train:\n",
    "    t=(tfidf[dictionary.doc2bow(text.lower().split())])\n",
    "    for key,item in t:\n",
    "        train[count][key]=item\n",
    "    count=count+1\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test=np.zeros((len(X_test),len(dictionary)))\n",
    "count1=0\n",
    "for text in X_test:\n",
    "    t=(tfidf[dictionary.doc2bow(text.lower().split())])\n",
    "    for key,item in t:\n",
    "        test[count1][key]=item\n",
    "    count1=count1+1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694, 4140)\n",
      "(6242, 4140)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48894585068888174\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(train,y_train)  \n",
    "x=clf.predict(test)\n",
    "print(clf.score(train,y_train))\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]=="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
