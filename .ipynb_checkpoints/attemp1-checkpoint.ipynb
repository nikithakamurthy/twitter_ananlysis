{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from techniques import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8  3 11 ... 11  9  9]\n",
      "['INFJ' 'ENTP' 'INTP' ... 'INTP' 'INFP' 'INFP']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename=\"mbti_1.csv\"\n",
    "arr=[]\n",
    "with open(filename,'rt',encoding='latin-1') as file:     \n",
    "            samples=csv.reader(file)\n",
    "            x=0\n",
    "            for i in samples:\n",
    "                if x==0:\n",
    "                    x=1\n",
    "                    continue\n",
    "                arr.append(i)\n",
    "df=pd.DataFrame(data=arr,columns=[\"types\",\"posts\"])\n",
    "#print(df.info())\n",
    "data=df['types']\n",
    "values=array(data)\n",
    "label=LabelEncoder()\n",
    "intencode=label.fit_transform(values)\n",
    "print(intencode)\n",
    "df['typeint']=intencode\n",
    "df['typeint'].plot(kind='hist')\n",
    "k=np.arange(0,16)\n",
    "x=label.inverse_transform(k)   #can access encoded actual value using x\n",
    "print(values)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  types                                              posts  typeint  \\\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...        8   \n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...        3   \n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...       11   \n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...       10   \n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...        2   \n",
      "\n",
      "   emojicount  slangcount  multipuktcount  urlcount  #_@count  \n",
      "0         0.0         0.0             0.0       0.0       0.0  \n",
      "1         0.0         0.0             0.0       0.0       0.0  \n",
      "2         0.0         0.0             0.0       0.0       0.0  \n",
      "3         0.0         0.0             0.0       0.0       0.0  \n",
      "4         0.0         0.0             0.0       0.0       0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "shortdata=df.head()        #replace all short_data to df.. after completion\n",
    "\n",
    "f=\"slang.txt\"\n",
    "a=[]\n",
    "with open(f,'rt',encoding='latin-1') as file:     \n",
    "            samples=csv.reader(file,delimiter='\\t')\n",
    "            x=0\n",
    "            for i in samples:\n",
    "                if x==0:\n",
    "                    x=1\n",
    "                    continue\n",
    "                a.append(i)\n",
    "slangframe=pd.DataFrame(data=a,columns=['slang','fullform'])\n",
    "shortdata['emojicount']=np.zeros(len(shortdata))\n",
    "shortdata['slangcount']=np.zeros(len(shortdata))\n",
    "shortdata['multipuktcount']=np.zeros(len(shortdata))\n",
    "shortdata['urlcount']=np.zeros(len(shortdata))\n",
    "shortdata['#_@count']=np.zeros(len(shortdata))\n",
    "print(shortdata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-f336a17cb83e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplaceNegations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#remove unicodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremoveUnicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;31m#removes numbers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremoveNumbers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\twitter_analysis\\attemp1\\techniques.py\u001b[0m in \u001b[0;36mremoveUnicode\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremoveUnicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;34m\"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'(\\\\u[0-9A-Fa-f]+)'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mr''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^\\x00-\\x7f]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mr''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "slang=list(slangframe['slang'])\n",
    "for i in range(len(shortdata)):\n",
    "    #counts emojis and remove\n",
    "    shortdata['emojicount'][i]=countEmoticons(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=removeEmoticons(shortdata['posts'][i])\n",
    "    #counts slang and removes\n",
    "    #shortdata['slangcount'][i]=countSlang(shortdata['posts'][i])\n",
    "    q=shortdata['posts'][i].split()\n",
    "    for j in range(len(q)):\n",
    "        if q[i] in slang:\n",
    "            q[i]=slangrame['fullform'][slang.index(q[i])]\n",
    "    shortdata['posts'][i]=' '.join(q)\n",
    "    #replaces contractions with full forms\n",
    "    shortdata['posts'][i]=replaceContraction(shortdata['posts'][i])\n",
    "    #counts urls and replaces\n",
    "    #shortdata['urlcount'][i]=countURL(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=replaceURL(shortdata['posts'][i]) \n",
    "    #counts #()-() and @-atuser and removes them\n",
    "    #shortdata['#_@count'][i]=counthashAt(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=replaceAtUser(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=removeHashtagInFrontOfWord(shortdata['posts'][i])\n",
    "    #counts punctuations and replacethem\n",
    "    #shortdata['multipuktcount'][i]=countPunct(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=replaceMultiExclamationMark(shortdata['posts'][i])  \n",
    "    shortdata['posts'][i]=replaceMultiQuestionMark(shortdata['posts'][i])  \n",
    "    shortdata['posts'][i]=replaceMultiStopMark(shortdata['posts'][i])  \n",
    "    #replaces negations\n",
    "    shortdata['posts'][i]=replaceNegations(shortdata['posts'][i])  \n",
    "    #remove unicodes\n",
    "    shortdata['posts'][i]=removeUnicode(shortdata['posts'][i])  \n",
    "    #removes numbers\n",
    "    shortdata['posts'][i]=removeNumbers(shortdata['posts'][i])\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words(\"english\")\n",
    "my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm httpurl httpsurl\" # my extra stopwords\n",
    "stop = stop + my_stopwords.split()\n",
    "shortdata['posts']=shortdata['posts'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "ps = PorterStemmer()\n",
    "shortdata['posts'] = shortdata['posts'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "shortdata['posts'] = shortdata['posts'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "shortdata\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-f8a31336ee54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print('------Removing stopwords------')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-f8a31336ee54>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#print('------Removing stopwords------')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshortdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words(\"english\")\n",
    "my_stopwords = \"multiexclamation multiquestion multistop url atuser st rd nd th am pm\" # my extra stopwords\n",
    "stop = stop + my_stopwords.split()\n",
    "\n",
    "#print('------Removing stopwords------')\n",
    "shortdata['posts']=shortdata['posts'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "ps = PorterStemmer()\n",
    "shortdata['posts'] = shortdata['posts'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "shortdata['posts'] = shortdata['posts'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "\n",
    "slang=list(slangframe['slang'])\n",
    "for i in range(len(shortdata)):\n",
    "    \n",
    "    shortdata['posts'][i]=replaceURL(shortdata['posts'][i])   #removing urls and replacing with \"url\"\n",
    "    shortdata['posts'][i]=removeUnicode(shortdata['posts'][i])   #removing unicodes\n",
    "    shortdata['posts'][i]=replaceAtUser(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=removeHashtagInFrontOfWord(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=removeNumbers(shortdata['posts'][i])\n",
    "    shortdata['posts'][i]=replaceAtUser(shortdata['posts'][i])\n",
    "    q=shortdata['posts'][i].split()\n",
    "    for j in range(len(q)):\n",
    "        if q[i] in slang:\n",
    "            q[i]=slangrame['fullform'][slang.index(q[i])]\n",
    "    shortdata['posts'][i]=' '.join(q)\n",
    "print(shortdata['posts'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from techniques import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=\"2nite im going wtf \"\n",
    "slang=list(slangframe['slang'])\n",
    "x=y.split()\n",
    "for i in range(len(x)):\n",
    "    if x[i] in slang:\n",
    "        ind=slang.index(x[i])\n",
    "        x[i]=slangframe['fullform'][ind]\n",
    "x=' '.join(x)\n",
    "x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=661, size=100, alpha=0.025)\n",
      "[ 4.3089041e-03  2.6918594e-03 -4.1790437e-03  5.0933077e-03\n",
      "  3.0020357e-03  1.5297990e-03  2.9358114e-04 -2.9847315e-03\n",
      "  1.3447115e-03  3.2404299e-05 -1.4441735e-03 -1.9040983e-03\n",
      "  2.3708788e-03  2.7327195e-03 -2.1859430e-04  8.0010825e-04\n",
      " -4.7453064e-03 -1.6376108e-04  3.3617930e-03 -3.1013184e-03\n",
      " -4.3208068e-03 -4.2986163e-04  3.9598863e-03  3.8732621e-03\n",
      " -3.2404326e-03  3.4980604e-03 -4.2973515e-03  1.9813050e-03\n",
      " -3.9066859e-03 -1.1866629e-03 -1.3926033e-03 -1.8774110e-03\n",
      " -1.9518785e-03 -3.1339212e-03 -4.9680113e-03 -4.6952576e-03\n",
      "  3.4290969e-03 -4.1419752e-03  1.2207129e-03  3.6175125e-03\n",
      "  4.6152966e-03  2.6541220e-03 -1.8823189e-03 -1.5989823e-03\n",
      " -4.8356457e-03  6.5808010e-04  3.9244685e-03 -4.5641302e-03\n",
      " -2.7857509e-03 -1.1214372e-03  1.8931538e-03 -4.3392456e-03\n",
      " -1.5461131e-03 -2.4799823e-03 -4.7145470e-04 -2.4719767e-03\n",
      " -3.9895428e-03  1.0624379e-03  1.9679442e-03 -9.5221406e-04\n",
      "  3.4363372e-03  2.8413881e-03 -1.2503054e-03  2.5285296e-03\n",
      " -4.1622538e-03 -2.6263567e-03  2.3352350e-03  3.8702744e-03\n",
      " -2.1570101e-03  2.4573924e-03 -3.4657470e-03  4.1105850e-03\n",
      " -3.6862867e-03 -8.5950916e-04  5.0666179e-03 -4.3087383e-03\n",
      "  3.0893164e-03  1.7798561e-04 -3.3170446e-03  2.7292869e-03\n",
      " -3.8464146e-03 -4.1265506e-03  5.2265293e-04 -1.2070441e-03\n",
      "  3.2169763e-03 -3.4471060e-04 -5.0834613e-03  4.1621272e-03\n",
      " -1.8224488e-04  4.0466762e-03 -2.4614364e-03 -6.4671319e-04\n",
      " -3.7709372e-03 -2.8493195e-03  1.2500561e-03 -4.4058650e-03\n",
      " -4.0557673e-03 -1.9782502e-03 -3.2909284e-03  1.5093694e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:142: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "c:\\users\\nikitha\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:155: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import nltk,sys\n",
    "import numpy as np,pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk.data\n",
    "import csv\n",
    "from nltk.stem.porter import *\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data = pd.read_csv(\"mbti_1.csv\") \n",
    "shortdata=data.iloc[:,-1]\n",
    "\n",
    "#print('-----Data-------')\n",
    "#print(shortdata)\n",
    "\n",
    "#removing stopwords \n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words(\"english\"),'I'\n",
    "#print('------Removing stopwords------')\n",
    "shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word!='i' or word!='I']))\n",
    "#print(shortdata)\n",
    "#stemming of words\n",
    "ps = PorterStemmer()\n",
    "#print('-------Stemming--------')\n",
    "shortdata = shortdata.apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "#print(shortdata)\n",
    "\n",
    "#removing non-alphabets\n",
    "shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "#print(shortdata)\n",
    "#print('-------Lemmatization--------')\n",
    "shortdata = shortdata.apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "#print(shortdata)\n",
    "\n",
    "\n",
    "#print('--------Removing punctuations--------')\n",
    "def clear_punctuation(s):\n",
    "\timport string\n",
    "\t#print(\"\\n\")\n",
    "\tclear_string = \"\"\n",
    "\tfor symbol in s:\n",
    "\t\tif symbol not in string.punctuation:\n",
    "\t\t\tclear_string += symbol\n",
    "\treturn clear_string\n",
    "\n",
    "shortdata = shortdata.apply(lambda x: ''.join(clear_punctuation(x))  ) \n",
    "#for line in shortdata:\n",
    "\t#print(line)\n",
    "\t#print('\\n')\n",
    "\n",
    "def strip_all_entities(text):\n",
    "\timport string\n",
    "\tentity_prefixes = ['@']\n",
    "\tfor separator in  string.punctuation:\n",
    "\t\tif separator not in entity_prefixes :\n",
    "\t\t\ttext = text.replace(separator,' ')\n",
    "\twords = []\n",
    "\tfor word in text.split():\n",
    "\t\tword = word.strip()\n",
    "\t\tif word:\n",
    "\t\t\tif word[0] not in entity_prefixes:\n",
    "\t\t\t\twords.append(word)\n",
    "\treturn ' '.join(words)\n",
    "\n",
    "shortdata = shortdata.apply(lambda x: ''.join(strip_all_entities(x))  ) \n",
    "#print(shortdata)\n",
    "\n",
    "#computing tfidf\n",
    "trainset=shortdata.iloc[0:3]\n",
    "#print('-------Train set-------')\n",
    "#print(trainset)\n",
    "testset=shortdata.iloc[3:5]\n",
    "#print('--------Test set--------')\n",
    "#print(testset)\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in trainset]\n",
    "#print(texts)\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "#print(frequency)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "#print(len(frequency))\n",
    "\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "#print(processed_corpus)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "#print(dictionary)\n",
    "#print('\\n')\n",
    "#print(dictionary)\n",
    "#print('\\n')\n",
    "#print(dictionary.token2id)\n",
    "#print('\\n')\n",
    "#word counts\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "\n",
    "\n",
    "#print('------Token ID and thier tf-idf weightings---------')\n",
    "from gensim import models\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "# transform the testset \n",
    "#for text in testset:\n",
    "\t#print(tfidf[dictionary.doc2bow(text.lower().split())])\n",
    "\t#print('\\n')\n",
    "    \n",
    "#Word Embedding Model\n",
    "sentences = []\n",
    "for line in trainset:\n",
    "\t#print (line.split())\n",
    "\tsentences.append(line.split())\n",
    "#print (sentences)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "#print (\"--------Vocabulary----------\")\n",
    "#print(words)\n",
    "# access vector for one word\n",
    "#print('\\n')\n",
    "#print (\"-----Embedding matrix--------\")\n",
    "print(model['weird'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "#print (\"-----Load model----\")\n",
    "#print(new_model)\n",
    "\n",
    "with open(\"Word_Embeddings.csv\",\"a\") as file1:\n",
    "\twriter = csv.writer(file1)\n",
    "\tfor word in words:\n",
    "\t\t#print(model[word])\n",
    "\t\twriter.writerow([word])\n",
    "\t\twriter.writerow(model[word])\n",
    "\n",
    "with open(\"TFIDF.csv\",\"a\") as file2:\n",
    "\twriter = csv.writer(file2)\n",
    "\tfor text in testset:\n",
    "\t\twriter.writerow(tfidf[dictionary.doc2bow(text.lower().split())])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
