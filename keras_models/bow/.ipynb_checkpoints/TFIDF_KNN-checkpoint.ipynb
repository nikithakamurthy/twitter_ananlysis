{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk.data\n",
    "import csv\n",
    "from nltk.stem.porter import *\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import array as arr\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data from dataset\n",
    "#data = pd.read_csv(\"mbti_1.csv\") \n",
    "#shortdata=data.iloc[:,-1]\n",
    "arr=[]\n",
    "file = open(\"downloaded1.csv\",'rt')\n",
    "samples=csv.reader(file)\n",
    "c=0\n",
    "for i in samples:\n",
    "    c+=1\n",
    "    \n",
    "    if c==2:\n",
    "        x=i[1]\n",
    "        break\n",
    "\n",
    "for i in samples:\n",
    "    if i[1]!=x:\n",
    "        arr.append(i)\n",
    "\n",
    "data=arr\n",
    "df=pd.DataFrame(data=arr,columns=(\"types\",\"posts\"))\n",
    "#print(len(df.columns))\n",
    "#print(df)\n",
    "#shortdata=shortdata.head()\n",
    "#shortdata1=data.iloc[0:5,0]\n",
    "#print('-----Data-------')\n",
    "#print(shortdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "def labelencode(df):\n",
    "    data=df['types']\n",
    "    values=np.array(data)\n",
    "    label=LabelEncoder()\n",
    "    intencode=label.fit_transform(values)\n",
    "    df['typeint']=intencode\n",
    "    print(list(label.inverse_transform([0,1,2])))\n",
    "    #df['typeint'].plot(kind='hist')\n",
    "    #k=np.arange(0,16)\n",
    "    #x=label.inverse_transform(k)   #can access encoded actual value using x\n",
    "    #print(values)\n",
    "    return df\n",
    "\n",
    "df=labelencode(df)\n",
    "#print(df)\n",
    "shortdata=df.iloc[:,1]\n",
    "#print(shortdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Removing stopwords------\n",
      "-------Stemming--------\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords \n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words(\"english\")\n",
    "print('------Removing stopwords------')\n",
    "shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word!='i' or word!='I']))\n",
    "#print(shortdata)\n",
    "#stemming of words\n",
    "ps = PorterStemmer()\n",
    "print('-------Stemming--------')\n",
    "shortdata = shortdata.apply(lambda x: ' '.join([ps.stem(word) for word in x.split() ]))\n",
    "#print(shortdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Lemmatization--------\n",
      "--------Removing punctuations--------\n"
     ]
    }
   ],
   "source": [
    "#removing non-alphabets\n",
    "shortdata=shortdata.apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "#print(shortdata)\n",
    "print('-------Lemmatization--------')\n",
    "shortdata = shortdata.apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split() ]))\n",
    "#print(shortdata)\n",
    "\n",
    "print('--------Removing punctuations--------')\n",
    "def clear_punctuation(s):\n",
    "\timport string\n",
    "\t#print(\"\\n\")\n",
    "\tclear_string = \"\"\n",
    "\tfor symbol in s:\n",
    "\t\tif symbol not in string.punctuation:\n",
    "\t\t\tclear_string += symbol\n",
    "\treturn clear_string\n",
    "\n",
    "shortdata = shortdata.apply(lambda x: ''.join(clear_punctuation(x))  )\n",
    "#print(shortdata)\n",
    "#for line in shortdata:\n",
    "#\tprint(line)\n",
    "#\tprint('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_all_entities(text):\n",
    "\timport string\n",
    "\tentity_prefixes = ['@']\n",
    "\tfor separator in  string.punctuation:\n",
    "\t\tif separator not in entity_prefixes :\n",
    "\t\t\ttext = text.replace(separator,' ')\n",
    "\twords = []\n",
    "\tfor word in text.split():\n",
    "\t\tword = word.strip()\n",
    "\t\tif word:\n",
    "\t\t\tif word[0] not in entity_prefixes:\n",
    "\t\t\t\twords.append(word)\n",
    "\treturn ' '.join(words)\n",
    "\n",
    "shortdata = shortdata.apply(lambda x: ''.join(strip_all_entities(x))  ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----PREPROCESSED_DATA------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for column in dataset.columns:\\n    if dataset[column].dtype == type(object):\\n        le = LabelEncoder()\\n        dataset[column] = le.fit_transform(dataset[column])'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "arr=[]\n",
    "print(\"-----PREPROCESSED_DATA------\")\n",
    "count=0\n",
    "for line in shortdata:\n",
    "    df.iloc[i,1]=line\n",
    "    i=i+1\n",
    "#print(df)\n",
    "\n",
    "\n",
    "\n",
    "'''for column in dataset.columns:\n",
    "    if dataset[column].dtype == type(object):\n",
    "        le = LabelEncoder()\n",
    "        dataset[column] = le.fit_transform(dataset[column])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936\n",
      "6936\n"
     ]
    }
   ],
   "source": [
    "proc_data=np.array(df['posts'])\n",
    "label=np.array(df['typeint'])\n",
    "print(len(proc_data))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(proc_data, label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gladli chum bikeless taysid free tuesday flexibl'\n",
      " 'I know one thing I write blog post newli discov love vampir'\n",
      " 'RT great practic today good rnd time lunch' ...\n",
      " 'lmao RT they make new friday kevin hart katt william and mike epp'\n",
      " 'sex offend warn may face jail offenc'\n",
      " 'depend unoffici your may seem progress too bad lead suppli heat']\n"
     ]
    }
   ],
   "source": [
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 5000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='tfidf')\n",
    "# treat the labels as categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = keras.utils.to_categorical(train_y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "sgd=keras.optimizers.SGD(lr=0.5, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer=sgd,\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4993 samples, validate on 555 samples\n",
      "Epoch 1/5\n",
      "4993/4993 [==============================] - 3s 571us/step - loss: 0.9846 - acc: 0.5606 - val_loss: 3.3216 - val_acc: 0.2234\n",
      "Epoch 2/5\n",
      "4993/4993 [==============================] - 3s 535us/step - loss: 0.7159 - acc: 0.7492 - val_loss: 1.1860 - val_acc: 0.6000\n",
      "Epoch 3/5\n",
      "4993/4993 [==============================] - 3s 544us/step - loss: 0.3736 - acc: 0.8800 - val_loss: 1.3690 - val_acc: 0.5874\n",
      "Epoch 4/5\n",
      "4993/4993 [==============================] - 3s 538us/step - loss: 0.2740 - acc: 0.9291 - val_loss: 1.4711 - val_acc: 0.6162\n",
      "Epoch 5/5\n",
      "4993/4993 [==============================] - 3s 532us/step - loss: 0.2205 - acc: 0.9509 - val_loss: 1.6281 - val_acc: 0.6072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa191e2cef0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "  batch_size=32,\n",
    "  epochs=5,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "# for human-friendly printing\n",
    "labels = ['negative','neutral','positive']\n",
    "\n",
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file:\n",
    "    dictionary = json.load(dictionary_file)\n",
    "\n",
    "# this utility makes sure that all the words in your input\n",
    "# are registered in the dictionary\n",
    "# before trying to turn them into a matrix.\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices\n",
    "\n",
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model.h5')\n",
    "\n",
    "# okay here's the interactive part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'are' not in training corpus; ignoring.\n",
      "positive sentiment; 41.023979% confidence\n"
     ]
    }
   ],
   "source": [
    "eval=\"hello how are you?\"\n",
    "testArr = convert_text_to_index_array(eval)\n",
    "inp = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "# predict which bucket your input belongs in\n",
    "pred = model.predict(inp)\n",
    "# and print it for the humons\n",
    "print(\"%s sentiment; %f%% confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tryna go north iowa bad accid'\n",
      " 'thi week mark anniversari death elvi elvi die memphi august'\n",
      " 'sinc ET go via bkk freedom right indirect' ...\n",
      " 'and watch degrassi friday looool awkward campbel'\n",
      " 'My sister I go SM saturday afternoon watch I ask dad popcorn'\n",
      " 'free psn game psn game zen pinbal free game playstat As oct get usa psn']\n"
     ]
    }
   ],
   "source": [
    "print(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'accid' not in training corpus; ignoring.\n",
      "[0.18361601 0.4163396  0.40004435]\n",
      "0.4163396\n",
      "neutral\n",
      "'memphi' not in training corpus; ignoring.\n",
      "[0.15507302 0.43749395 0.40743303]\n",
      "0.43749395\n",
      "neutral\n",
      "'indirect' not in training corpus; ignoring.\n",
      "[0.1756502  0.4023764  0.42197338]\n",
      "0.42197338\n",
      "positive\n",
      "[0.17176639 0.43324065 0.39499304]\n",
      "0.43324065\n",
      "neutral\n",
      "'leia' not in training corpus; ignoring.\n",
      "[0.17186326 0.4245973  0.4035395 ]\n",
      "0.4245973\n",
      "neutral\n",
      "'rockwel' not in training corpus; ignoring.\n",
      "'tuyo' not in training corpus; ignoring.\n",
      "[0.13672978 0.45661265 0.40665764]\n",
      "0.45661265\n",
      "neutral\n",
      "'visafon' not in training corpus; ignoring.\n",
      "'lago' not in training corpus; ignoring.\n",
      "'cdma' not in training corpus; ignoring.\n",
      "[0.09488693 0.60276914 0.30234388]\n",
      "0.60276914\n",
      "neutral\n",
      "[0.17344986 0.39850265 0.4280475 ]\n",
      "0.4280475\n",
      "positive\n",
      "[0.16863956 0.4300043  0.40135616]\n",
      "0.4300043\n",
      "neutral\n",
      "[0.22990653 0.3506808  0.4194126 ]\n",
      "0.4194126\n",
      "positive\n",
      "[0.1923803  0.33074751 0.47687212]\n",
      "0.47687212\n",
      "positive\n",
      "'flyyyy' not in training corpus; ignoring.\n",
      "[0.17017137 0.43134165 0.39848697]\n",
      "0.43134165\n",
      "neutral\n",
      "'dismiss' not in training corpus; ignoring.\n",
      "[0.14028925 0.4724015  0.38730925]\n",
      "0.4724015\n",
      "neutral\n",
      "[0.12566526 0.43466377 0.43967092]\n",
      "0.43967092\n",
      "positive\n",
      "'yest' not in training corpus; ignoring.\n",
      "[0.15355007 0.4174629  0.4289871 ]\n",
      "0.4289871\n",
      "positive\n",
      "'ponzi' not in training corpus; ignoring.\n",
      "[0.18313359 0.40110523 0.41576123]\n",
      "0.41576123\n",
      "positive\n",
      "'ae' not in training corpus; ignoring.\n",
      "[0.14177255 0.4518045  0.40642297]\n",
      "0.4518045\n",
      "neutral\n",
      "'su' not in training corpus; ignoring.\n",
      "[0.162609   0.43338314 0.4040079 ]\n",
      "0.43338314\n",
      "neutral\n",
      "[0.04258909 0.07882363 0.87858725]\n",
      "0.87858725\n",
      "positive\n",
      "'xavier' not in training corpus; ignoring.\n",
      "[0.10553371 0.47725555 0.4172108 ]\n",
      "0.47725555\n",
      "neutral\n",
      "[0.08115819 0.5465062  0.37233564]\n",
      "0.5465062\n",
      "neutral\n",
      "'maldini' not in training corpus; ignoring.\n",
      "[0.15911071 0.48271102 0.35817826]\n",
      "0.48271102\n",
      "neutral\n",
      "[0.14867236 0.39328867 0.45803896]\n",
      "0.45803896\n",
      "positive\n",
      "[0.13615328 0.35380644 0.5100402 ]\n",
      "0.5100402\n",
      "positive\n",
      "'mcbee' not in training corpus; ignoring.\n",
      "[0.15980591 0.43271065 0.4074835 ]\n",
      "0.43271065\n",
      "neutral\n",
      "[0.1655338  0.32735184 0.5071144 ]\n",
      "0.5071144\n",
      "positive\n",
      "[0.17427558 0.35194215 0.4737822 ]\n",
      "0.4737822\n",
      "positive\n",
      "[0.17207883 0.39030242 0.4376188 ]\n",
      "0.4376188\n",
      "positive\n",
      "'yahoo' not in training corpus; ignoring.\n",
      "'scoreboard' not in training corpus; ignoring.\n",
      "'cincinnati' not in training corpus; ignoring.\n",
      "[0.17694187 0.43697357 0.3860845 ]\n",
      "0.43697357\n",
      "neutral\n",
      "'capella' not in training corpus; ignoring.\n",
      "'earthski' not in training corpus; ignoring.\n",
      "[0.19487499 0.40196902 0.403156  ]\n",
      "0.403156\n",
      "positive\n",
      "'oven' not in training corpus; ignoring.\n",
      "'fell' not in training corpus; ignoring.\n",
      "[0.16831759 0.32547078 0.5062117 ]\n",
      "0.5062117\n",
      "positive\n",
      "'suryakumar' not in training corpus; ignoring.\n",
      "'yadav' not in training corpus; ignoring.\n",
      "[0.11829864 0.42515197 0.4565494 ]\n",
      "0.4565494\n",
      "positive\n",
      "'outtak' not in training corpus; ignoring.\n",
      "[0.16578092 0.42795426 0.4062648 ]\n",
      "0.42795426\n",
      "neutral\n",
      "[0.10788126 0.32343367 0.5686851 ]\n",
      "0.5686851\n",
      "positive\n",
      "'ll' not in training corpus; ignoring.\n",
      "[0.15464948 0.45471734 0.3906332 ]\n",
      "0.45471734\n",
      "neutral\n",
      "'cleverli' not in training corpus; ignoring.\n",
      "'sneijder' not in training corpus; ignoring.\n",
      "[0.15377304 0.33440855 0.5118184 ]\n",
      "0.5118184\n",
      "positive\n",
      "[0.17640223 0.41197544 0.41162223]\n",
      "0.41197544\n",
      "neutral\n",
      "[0.17059699 0.38284302 0.44656   ]\n",
      "0.44656\n",
      "positive\n",
      "'chp' not in training corpus; ignoring.\n",
      "[0.1360778 0.2967959 0.5671262]\n",
      "0.5671262\n",
      "positive\n",
      "[0.13181843 0.43650872 0.4316728 ]\n",
      "0.43650872\n",
      "neutral\n",
      "[0.16663055 0.44134173 0.39202777]\n",
      "0.44134173\n",
      "neutral\n",
      "[0.29615816 0.3624158  0.34142604]\n",
      "0.3624158\n",
      "neutral\n",
      "'eateri' not in training corpus; ignoring.\n",
      "'trivia' not in training corpus; ignoring.\n",
      "'darien' not in training corpus; ignoring.\n",
      "[0.13076295 0.3339795  0.5352576 ]\n",
      "0.5352576\n",
      "positive\n",
      "'granddaught' not in training corpus; ignoring.\n",
      "'bucksadschool' not in training corpus; ignoring.\n",
      "[0.16527103 0.3135939  0.52113515]\n",
      "0.52113515\n",
      "positive\n",
      "[0.18197635 0.42504987 0.39297384]\n",
      "0.42504987\n",
      "neutral\n",
      "'hicksvil' not in training corpus; ignoring.\n",
      "[0.17293428 0.47679633 0.3502694 ]\n",
      "0.47679633\n",
      "neutral\n",
      "[0.23472306 0.3525428  0.41273412]\n",
      "0.41273412\n",
      "positive\n",
      "[0.15489246 0.3947266  0.45038083]\n",
      "0.45038083\n",
      "positive\n",
      "[0.13912961 0.53595173 0.32491872]\n",
      "0.53595173\n",
      "neutral\n",
      "'tide' not in training corpus; ignoring.\n",
      "'wcw' not in training corpus; ignoring.\n",
      "[0.15782078 0.4409868  0.40119246]\n",
      "0.4409868\n",
      "neutral\n",
      "[0.18919966 0.32343522 0.48736522]\n",
      "0.48736522\n",
      "positive\n",
      "'goodger' not in training corpus; ignoring.\n",
      "[0.14899367 0.53949267 0.3115136 ]\n",
      "0.53949267\n",
      "neutral\n",
      "'swindon' not in training corpus; ignoring.\n",
      "[0.15323348 0.38090196 0.46586463]\n",
      "0.46586463\n",
      "positive\n",
      "'horton' not in training corpus; ignoring.\n",
      "[0.15899509 0.44289792 0.39810696]\n",
      "0.44289792\n",
      "neutral\n",
      "[0.06159772 0.58614373 0.3522586 ]\n",
      "0.58614373\n",
      "neutral\n",
      "'udfa' not in training corpus; ignoring.\n",
      "'brandian' not in training corpus; ignoring.\n",
      "[0.17630662 0.41669247 0.4070009 ]\n",
      "0.41669247\n",
      "neutral\n",
      "[0.14479204 0.48122293 0.373985  ]\n",
      "0.48122293\n",
      "neutral\n",
      "'panjab' not in training corpus; ignoring.\n",
      "'meteorolog' not in training corpus; ignoring.\n",
      "'forestri' not in training corpus; ignoring.\n",
      "[0.15503123 0.45196643 0.39300233]\n",
      "0.45196643\n",
      "neutral\n",
      "'heartbreak' not in training corpus; ignoring.\n",
      "'ao' not in training corpus; ignoring.\n",
      "[0.15608788 0.43656275 0.40734944]\n",
      "0.43656275\n",
      "neutral\n",
      "[0.0604463  0.7806048  0.15894887]\n",
      "0.7806048\n",
      "neutral\n",
      "[0.15066968 0.49079844 0.35853186]\n",
      "0.49079844\n",
      "neutral\n",
      "[0.15274718 0.32444814 0.5228046 ]\n",
      "0.5228046\n",
      "positive\n",
      "[0.18537869 0.402272   0.41234934]\n",
      "0.41234934\n",
      "positive\n",
      "[0.14132977 0.40858963 0.45008057]\n",
      "0.45008057\n",
      "positive\n",
      "[0.14116317 0.3031169  0.5557199 ]\n",
      "0.5557199\n",
      "positive\n",
      "[0.16617045 0.45828518 0.3755444 ]\n",
      "0.45828518\n",
      "neutral\n",
      "[0.16530544 0.43882814 0.39586645]\n",
      "0.43882814\n",
      "neutral\n",
      "'uppc' not in training corpus; ignoring.\n",
      "[0.17567672 0.38008648 0.44423676]\n",
      "0.44423676\n",
      "positive\n",
      "[0.13325378 0.52130806 0.34543818]\n",
      "0.52130806\n",
      "neutral\n",
      "[0.13901417 0.4667373  0.3942485 ]\n",
      "0.4667373\n",
      "neutral\n",
      "[0.09677788 0.19637048 0.70685166]\n",
      "0.70685166\n",
      "positive\n",
      "[0.18618312 0.40693778 0.40687907]\n",
      "0.40693778\n",
      "neutral\n",
      "'surrey' not in training corpus; ignoring.\n",
      "[0.16503094 0.46262228 0.37234676]\n",
      "0.46262228\n",
      "neutral\n",
      "[0.1393714  0.41969213 0.44093657]\n",
      "0.44093657\n",
      "positive\n",
      "'syrian' not in training corpus; ignoring.\n",
      "'withdraw' not in training corpus; ignoring.\n",
      "[0.18941477 0.38342863 0.42715663]\n",
      "0.42715663\n",
      "positive\n",
      "'cecil' not in training corpus; ignoring.\n",
      "[0.25685647 0.42193925 0.32120425]\n",
      "0.42193925\n",
      "neutral\n",
      "[0.14187819 0.42728257 0.43083924]\n",
      "0.43083924\n",
      "positive\n",
      "[0.17957012 0.4580992  0.3623307 ]\n",
      "0.4580992\n",
      "neutral\n",
      "'rhythm' not in training corpus; ignoring.\n",
      "[0.14156622 0.46556    0.3928738 ]\n",
      "0.46556\n",
      "neutral\n",
      "[0.16584589 0.41451257 0.41964146]\n",
      "0.41964146\n",
      "positive\n",
      "'czech' not in training corpus; ignoring.\n",
      "'luka' not in training corpus; ignoring.\n",
      "'rosol' not in training corpus; ignoring.\n",
      "'wimbledon' not in training corpus; ignoring.\n",
      "[0.17603837 0.49828625 0.3256754 ]\n",
      "0.49828625\n",
      "neutral\n",
      "'gerrard' not in training corpus; ignoring.\n",
      "[0.19038509 0.38771382 0.42190105]\n",
      "0.42190105\n",
      "positive\n",
      "'rhyme' not in training corpus; ignoring.\n",
      "[0.12771899 0.23934434 0.63293666]\n",
      "0.63293666\n",
      "positive\n",
      "[0.18211477 0.46745536 0.35042986]\n",
      "0.46745536\n",
      "neutral\n",
      "[0.10949387 0.43130007 0.45920604]\n",
      "0.45920604\n",
      "positive\n",
      "'awww' not in training corpus; ignoring.\n",
      "[0.11380058 0.286438   0.5997614 ]\n",
      "0.5997614\n",
      "positive\n",
      "[0.08525791 0.400553   0.5141891 ]\n",
      "0.5141891\n",
      "positive\n",
      "[0.06423473 0.10512496 0.83064026]\n",
      "0.83064026\n",
      "positive\n",
      "[0.16140607 0.38474602 0.45384791]\n",
      "0.45384791\n",
      "positive\n",
      "[0.16323958 0.40669283 0.4300675 ]\n",
      "0.4300675\n",
      "positive\n",
      "[0.09510742 0.25534785 0.6495448 ]\n",
      "0.6495448\n",
      "positive\n",
      "[0.1566038 0.4455728 0.3978235]\n",
      "0.4455728\n",
      "neutral\n",
      "[0.17523275 0.48613638 0.33863094]\n",
      "0.48613638\n",
      "neutral\n",
      "'lure' not in training corpus; ignoring.\n",
      "[0.17394541 0.4740263  0.35202825]\n",
      "0.4740263\n",
      "neutral\n",
      "'merril' not in training corpus; ignoring.\n",
      "'hoge' not in training corpus; ignoring.\n",
      "[0.15491115 0.4893009  0.355788  ]\n",
      "0.4893009\n",
      "neutral\n",
      "[0.18716967 0.33107883 0.4817515 ]\n",
      "0.4817515\n",
      "positive\n",
      "[0.16563514 0.46244636 0.37191844]\n",
      "0.46244636\n",
      "neutral\n",
      "'sexiest' not in training corpus; ignoring.\n",
      "[0.11293016 0.61088806 0.27618176]\n",
      "0.61088806\n",
      "neutral\n",
      "[0.16288406 0.44965625 0.3874597 ]\n",
      "0.44965625\n",
      "neutral\n",
      "[0.14699656 0.5933412  0.2596622 ]\n",
      "0.5933412\n",
      "neutral\n",
      "[0.11868699 0.5965396  0.2847734 ]\n",
      "0.5965396\n",
      "neutral\n",
      "'dash' not in training corpus; ignoring.\n",
      "[0.16735816 0.48555976 0.34708217]\n",
      "0.48555976\n",
      "neutral\n",
      "[0.17844291 0.39568555 0.42587155]\n",
      "0.42587155\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "que=[]\n",
    "for i in test_x:\n",
    "    #evalSentence = input('Input a sentence to be evaluated, or Enter to quit: ')\n",
    "    evalSentence=i\n",
    "    if len(evalSentence) == 0:\n",
    "        break\n",
    "\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(evalSentence)\n",
    "    inp = tokenizer.sequences_to_matrix([testArr], mode='')\n",
    "    # predict which bucket your input belongs in\n",
    "    pred = model.predict(inp)\n",
    "    k=np.amax(pred[0])\n",
    "    print(pred[0])\n",
    "    print(k)\n",
    "    ind=list(pred[0]).index(k)\n",
    "    print(labels[ind])\n",
    "    que.append(ind)\n",
    "    # and print it for the humons\n",
    "    #print(\"%s sentiment; %f%% confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5728155339805825\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "notp=0\n",
    "notn=0\n",
    "notq=0\n",
    "for i in range(len(que)):\n",
    "    if que[i]==test_y[i]:\n",
    "        count+=1\n",
    "print(count/len(que))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=pd.DataFrame(data=data,columns=(\"types\",\"posts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "newdf=labelencode(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6936\n",
      "6936\n"
     ]
    }
   ],
   "source": [
    "nposts=np.array(newdf['posts'])\n",
    "nlabels=np.array(newdf['typeint'])\n",
    "print(len(nposts))\n",
    "print(len(nlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['posts'] = newdf['posts'].apply(lambda x: x.lower())\n",
    "\n",
    "newdf['posts'] = newdf['posts'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 5000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(newdf['posts'].values)\n",
    "X = tokenizer.texts_to_sequences(newdf['posts'].values)\n",
    "X = sequence.pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 190, 128)          640000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 895,391\n",
      "Trainable params: 895,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "#model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5548, 190) (5548, 3)\n",
      "(1388, 190) (1388, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(newdf['types']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " - 34s - loss: 0.9483 - acc: 0.5442\n",
      "Epoch 2/7\n",
      " - 38s - loss: 0.6723 - acc: 0.7186\n",
      "Epoch 3/7\n",
      " - 40s - loss: 0.4609 - acc: 0.8198\n",
      "Epoch 4/7\n",
      " - 38s - loss: 0.3383 - acc: 0.8700\n",
      "Epoch 5/7\n",
      " - 39s - loss: 0.2544 - acc: 0.9093\n",
      "Epoch 6/7\n",
      " - 38s - loss: 0.1830 - acc: 0.9349\n",
      "Epoch 7/7\n",
      " - 39s - loss: 0.1413 - acc: 0.9537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79b6b1a898>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 1.64\n",
      "acc: 0.62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score,acc=(model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size))\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
